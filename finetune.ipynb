{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/lypan/anaconda3/envs/chatglm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-01-12 13:23:41,779] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 13:23:53.239472: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-01-12 13:23:53.286609: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-12 13:23:53.286647: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-12 13:23:53.286673: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-12 13:23:53.296403: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-12 13:23:54.198801: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, BloomForCausalLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/data/lypan/llm_interpre/neuron_info/bloom-560m/\"\n",
    "\n",
    "with open(folder_path + 'lang_agnos.json', 'r') as json_file:\n",
    "    lang_agnos = json.load(json_file)\n",
    "\n",
    "with open(folder_path + 'lang_speci_by_lang.json', 'r') as json_file:\n",
    "    lang_speci = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/data/lypan/llms/bloom-560m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = BloomForCausalLM.from_pretrained(model_path).to(\"cuda:2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_specific_param(model, module_name):\n",
    "    weight_name = module_name + \".weight\"\n",
    "    bias_name = module_name + \".bias\"\n",
    "    param_dict = {\"weight\": None, \"bias\": None}\n",
    "    for name, param in model.named_parameters():\n",
    "        if name == weight_name:\n",
    "            param_dict[\"weight\"] = param\n",
    "        elif name == bias_name:\n",
    "            param_dict[\"bias\"] = param\n",
    "\n",
    "    return param_dict\n",
    "\n",
    "def get_mask_grad(model, module_name):\n",
    "    \"\"\"返回单一指定结构的mask梯度\n",
    "\n",
    "    Args:\n",
    "        model (_type_): _description_\n",
    "        module_name (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        mask_grad: _description_\n",
    "    \"\"\"\n",
    "    # 寻找对应结构的weight和bias参数\n",
    "    param_dict = find_specific_param(model, module_name)\n",
    "    weight_grad, bias_grad = None, None\n",
    "    # 计算对应参数的梯度矩阵\n",
    "    if param_dict[\"weight\"] is not None:\n",
    "        weight_grad = param_dict[\"weight\"].grad\n",
    "    if param_dict[\"bias\"] is not None:\n",
    "        bias_grad = param_dict[\"bias\"].grad\n",
    "\n",
    "    # 计算当前结构神经元数量\n",
    "    # neuron_num = weight_grad.shape[0] if weight_grad is not None else bias_grad.shape[0]\n",
    "    # 初始化weight和bias的mask矩阵\n",
    "    mask_weight_matrix = torch.zeros_like(weight_grad) if weight_grad is not None else None\n",
    "    mask_bias_matrix = torch.zeros_like(bias_grad) if bias_grad is not None else None\n",
    "\n",
    "    lang_agnos_neuron_index = torch.tensor(lang_agnos[module_name])\n",
    "    \n",
    "    # TODO: 后续要加上语言特定神经元也需要保留\n",
    "    retain_index = lang_agnos_neuron_index\n",
    "\n",
    "    # 构造weight和bias的mask矩阵\n",
    "    if mask_weight_matrix is not None:\n",
    "        mask_weight_matrix[retain_index] = 1\n",
    "    if mask_bias_matrix is not None:\n",
    "        mask_bias_matrix[retain_index] = 1\n",
    "\n",
    "    # 进行mask操作\n",
    "    if weight_grad is not None:\n",
    "        weight_grad = weight_grad * mask_weight_matrix\n",
    "    if bias_grad is not None:\n",
    "        bias_grad = bias_grad * mask_bias_matrix\n",
    "\n",
    "    mask_grad = {\"mask_weight_grad\": weight_grad, \"mask_bias_grad\": bias_grad}\n",
    "\n",
    "    return mask_grad\n",
    "\n",
    "def get_all_mask_grad(model, module_name_list):\n",
    "    \"\"\"返回指定结构列表的mask梯度\n",
    "\n",
    "    Args:\n",
    "        model (_type_): _description_\n",
    "        module_name_list (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        all_mask_grad: _description_\n",
    "    \"\"\"\n",
    "    all_mask_grad = {}\n",
    "    for i in range(len(module_name_list)):\n",
    "        module_name = module_name_list[i]\n",
    "        mask_grad = get_mask_grad(model, module_name)\n",
    "        all_mask_grad[module_name + \".weight\"] = mask_grad[\"mask_weight_grad\"]\n",
    "        all_mask_grad[module_name + \".bias\"] = mask_grad[\"mask_bias_grad\"]\n",
    "    return all_mask_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 7/7 [03:25<00:00, 29.33s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3, Loss: 1.589456815054291e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 7/7 [00:01<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3, Loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 7/7 [00:01<00:00,  5.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3, Loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "module_name_list = ['transformer.h.0.mlp.dense_4h_to_h', 'transformer.h.1.mlp.dense_4h_to_h']\n",
    "\n",
    "input_text = [\"Your input text here.\"] * 50\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).input_ids.to(\"cuda:2\")\n",
    "\n",
    "inputs = input_ids[:, :-1]\n",
    "labels = input_ids[:, 1:]\n",
    "\n",
    "learning_rate = 5e-5\n",
    "batch_size = 8\n",
    "num_epochs = 3\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in tqdm(range(0, len(inputs), batch_size), desc=\"Epoch \"+ str(epoch)):\n",
    "        batch_inputs = inputs[i:i+batch_size]\n",
    "        batch_labels = labels[i:i+batch_size]\n",
    "\n",
    "        # 正向传播\n",
    "        outputs = model(batch_inputs, labels=batch_labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # 对梯度进行修改\n",
    "        all_mask_grad = get_all_mask_grad(model, module_name_list)\n",
    "\n",
    "        weight_name_list = [module_name + \".weight\" for module_name in module_name_list]\n",
    "        bias_name_list = [module_name + \".bias\" for module_name in module_name_list]\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in weight_name_list:\n",
    "                param.grad = all_mask_grad[name]\n",
    "            elif name in bias_name_list:\n",
    "                param.grad = all_mask_grad[name]\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatglm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
