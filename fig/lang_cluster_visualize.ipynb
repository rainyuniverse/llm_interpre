{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/lypan/anaconda3/envs/chatglm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-02-16 19:01:15,360] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-16 19:01:20.818416: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-16 19:01:20.907984: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-16 19:01:20.908052: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-16 19:01:20.908089: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-16 19:01:20.933715: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-16 19:01:22.558137: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "from transformers import AutoTokenizer, BloomForCausalLM\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import json\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_cache = []\n",
    "def find_all_target_modules(model):\n",
    "    target_module_name_list = []\n",
    "    for name, module in model.named_modules():\n",
    "        if len(module._parameters) > 0:\n",
    "            target_module_name_list.append(name)\n",
    "    return target_module_name_list\n",
    "\n",
    "def read_data(file_path):\n",
    "    sent_list = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file.readlines():\n",
    "            sent_list.append(line.replace(\"\\n\", \"\"))\n",
    "    return sent_list\n",
    "\n",
    "def read_monolingual_data(lang_code):\n",
    "    file_path = \"/data/lypan/peft/data/flores200_dataset/dev/\" + lang_code + \".dev\"\n",
    "    sent_list = read_data(file_path)\n",
    "    return sent_list\n",
    "\n",
    "def get_lang_agnos_speci_neuron(folder_path):\n",
    "    with open(folder_path + 'lang_agnos.json', 'r') as json_file:\n",
    "        lang_agnos = json.load(json_file)\n",
    "    with open(folder_path + 'lang_speci.json', 'r') as json_file:\n",
    "        lang_speci = json.load(json_file)\n",
    "\n",
    "    return lang_agnos, lang_speci\n",
    "\n",
    "def forward_hook(module, input, output, module_name):\n",
    "    # 在前向传播时调用\n",
    "    forward_cache.append(output)\n",
    "\n",
    "def add_hooks(model, target_module_names):\n",
    "    hook_forwards = []\n",
    "    for name, module in model.named_modules():\n",
    "        if name in target_module_names:\n",
    "            print(name, module)\n",
    "            handle_forward = module.register_forward_hook(lambda m, i, o, module_name=name: forward_hook(m, i, o, name))\n",
    "            hook_forwards.append(handle_forward)\n",
    "\n",
    "    return hook_forwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.h.10.mlp.dense_h_to_4h Linear(in_features=1024, out_features=4096, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997/997 [01:05<00:00, 15.20it/s]\n",
      "100%|██████████| 997/997 [01:06<00:00, 14.98it/s]\n",
      "100%|██████████| 997/997 [01:10<00:00, 14.07it/s]\n",
      "100%|██████████| 997/997 [01:10<00:00, 14.18it/s]\n",
      "100%|██████████| 997/997 [01:14<00:00, 13.39it/s]\n",
      "100%|██████████| 997/997 [01:16<00:00, 12.99it/s]\n",
      "100%|██████████| 997/997 [01:31<00:00, 10.85it/s]\n",
      "100%|██████████| 997/997 [01:36<00:00, 10.31it/s]\n",
      "100%|██████████| 997/997 [01:22<00:00, 12.10it/s]\n",
      "100%|██████████| 997/997 [01:08<00:00, 14.51it/s]\n"
     ]
    }
   ],
   "source": [
    "lang_code_list = ['arb_Arab', 'fra_Latn', 'spa_Latn', 'eng_Latn', 'deu_Latn', 'ita_Latn', 'jpn_Jpan', 'rus_Cyrl', 'zho_Hans', 'zho_Hant']\n",
    "\n",
    "model_path = \"/data/lypan/llms/bloom-560m\"\n",
    "neuron_info_path = \"/data/lypan/llm_interpre/neuron_info/bloom-560m/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "bloom = BloomForCausalLM.from_pretrained(model_path).to(\"cuda\")\n",
    "\n",
    "target_module_name_list = find_all_target_modules(bloom)\n",
    "# 目标结构名称列表\n",
    "target_module_names = [\"transformer.h.10.mlp.dense_h_to_4h\"]\n",
    "\n",
    "language_agnostic_representation = {key: [] for key in target_module_names}\n",
    "language_specific_representation = {key: [] for key in target_module_names}\n",
    "\n",
    "hook_forwards = add_hooks(bloom, target_module_names)\n",
    "\n",
    "repre_method = \"part\"\n",
    "\n",
    "language_agnostic_neurons, language_specific_neurons = get_lang_agnos_speci_neuron(neuron_info_path)\n",
    "\n",
    "for i in range(len(lang_code_list)):\n",
    "    cur_lang_code = lang_code_list[i]\n",
    "\n",
    "    sent_list = read_monolingual_data(cur_lang_code)\n",
    "\n",
    "    for j in tqdm(range(len(sent_list))):\n",
    "        text = sent_list[j]\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        input = inputs[\"input_ids\"][:, 0:len(inputs[\"input_ids\"][0]) - 1]\n",
    "        label = inputs[\"input_ids\"][:, 1:len(inputs[\"input_ids\"][0])]\n",
    "        outputs = bloom(input, labels=label)\n",
    "\n",
    "        for k in range(len(target_module_names)):\n",
    "            cur_module_name = target_module_names[k]\n",
    "\n",
    "            if repre_method == \"all\":\n",
    "                agnostic_length = len(language_specific_neurons[cur_module_name])\n",
    "                specific_length = len(language_agnostic_neurons[cur_module_name])\n",
    "                all_indices = torch.arange(agnostic_length + specific_length)\n",
    "\n",
    "                agnostic_bool_mask_matrix = torch.isin(all_indices, torch.tensor(language_specific_neurons[cur_module_name]))\n",
    "                specific_bool_mask_matrix = ~agnostic_bool_mask_matrix\n",
    "\n",
    "                agnostic_mask_matrix = torch.where(agnostic_bool_mask_matrix, torch.tensor(0), torch.tensor(1))\n",
    "                specific_mask_matrix = torch.where(specific_bool_mask_matrix, torch.tensor(0), torch.tensor(1))\n",
    "                # print(agnostic_mask_matrix)\n",
    "\n",
    "                language_agnostic_representation[cur_module_name].append(forward_cache[k].detach().cpu() * agnostic_mask_matrix)\n",
    "                language_specific_representation[cur_module_name].append(forward_cache[k].detach().cpu() * specific_mask_matrix)\n",
    "                    \n",
    "            elif repre_method == \"part\":\n",
    "                language_agnostic_representation[cur_module_name].append(forward_cache[k].detach().cpu().index_select(\n",
    "                        -1, torch.tensor(language_agnostic_neurons[target_module_names[k]])))\n",
    "                language_specific_representation[cur_module_name].append(forward_cache[k].detach().cpu().index_select(\n",
    "                        -1, torch.tensor(language_specific_neurons[target_module_names[k]])))\n",
    "\n",
    "        forward_cache = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_agnostic_repre = [tensor.mean(dim=1) for tensor in language_agnostic_representation[\"transformer.h.10.mlp.dense_h_to_4h\"]]\n",
    "agnostic_repre = torch.stack(mean_agnostic_repre)\n",
    "mean_agnostic_repre = agnostic_repre.view(-1, agnostic_repre.shape[-1]) # [lang_num * sent_num, lang_agnos_neuron_num]\n",
    "\n",
    "mean_specific_repre = [tensor.mean(dim=1) for tensor in language_specific_representation[\"transformer.h.10.mlp.dense_h_to_4h\"]]\n",
    "specific_repre = torch.stack(mean_specific_repre)\n",
    "mean_specific_repre = specific_repre.view(-1, specific_repre.shape[-1]) # [lang_num * sent_num, lang_speci_neuron_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 语言共有神经元表示聚类\n",
    "high_dimensional_data = mean_agnostic_repre\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=300)\n",
    "low_dimensional_data = tsne.fit_transform(high_dimensional_data)\n",
    "\n",
    "colors = ['#1abc9c', '#2ecc71', '#3498db', '#9b59b6', '#34495e', '#f1c40f', '#e67e22', '#e74c3c', '#5f27cd', '#95a5a6']\n",
    "plt.figure(figsize=(8,6))\n",
    "scatter_list = []\n",
    "for i in range((int)(len(low_dimensional_data) / 997)):\n",
    "    start = i * 997\n",
    "    end = (i + 1) * 997\n",
    "    scatter_list.append(plt.scatter(low_dimensional_data[start:end, 0], low_dimensional_data[start:end, 1], c=colors[i], s=2))\n",
    "# plt.title('lang_agnos neuron t-SNE Visualization')\n",
    "plt.legend(scatter_list, lang_code_list, ncol=2)\n",
    "plt.savefig('lang_agnos_cluster.pdf', format='pdf', dpi=300)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 语言特有神经元表示聚类\n",
    "high_dimensional_data = mean_specific_repre\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=20, n_iter=300)\n",
    "low_dimensional_data = tsne.fit_transform(high_dimensional_data)\n",
    "\n",
    "colors = ['#1abc9c', '#2ecc71', '#3498db', '#9b59b6', '#34495e', '#f1c40f', '#e67e22', '#e74c3c', '#5f27cd', '#95a5a6']\n",
    "plt.figure(figsize=(8,6))\n",
    "scatter_list = []\n",
    "for i in range((int)(len(low_dimensional_data) / 997)):\n",
    "    start = i * 997\n",
    "    end = (i + 1) * 997\n",
    "    scatter_list.append(plt.scatter(low_dimensional_data[start:end, 0], low_dimensional_data[start:end, 1], c=colors[i], s=2))\n",
    "# plt.title('lang_speci neuron t-SNE Visualization')\n",
    "plt.legend(scatter_list, lang_code_list, ncol=2)\n",
    "plt.savefig('lang_speci_cluster.pdf', format='pdf', dpi=300)\n",
    "plt.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatglm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
